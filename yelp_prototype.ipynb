{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from yelp.client import Client\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "import heapq  \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API_KEY = \"cv-xXEAMYyEV3UEF-AcDxBCH5TgNXveK9Gs_Sh9xITWYM6ob_6zKQYSCJKRMndCT6Flckrsj99lg_7GqSmoDDRYIr-3S4Dqe7CmfuwhSY_UAiYIQC_CP2QgpE4DTW3Yx\"\n",
    "\n",
    "# client = Client(API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract yelp reviews into reviews list\n",
    "\n",
    "reviews = []\n",
    "url = \"https://www.yelp.com/biz/kowloon-saugus-6\"\n",
    "for i in range(1,10):\n",
    "    result = requests.get(url)\n",
    "    html = result.text\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    review_div = soup.find_all('div', {'class':'review-content'})\n",
    "    for element in review_div:\n",
    "        p = element.find('p')\n",
    "        if p:\n",
    "            reviews.append(p.text)\n",
    "    url = \"https://www.yelp.com/biz/kowloon-saugus-6?start=\" + str(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment_words = ' '\n",
    "# stopwords = set(STOPWORDS) \n",
    "  \n",
    "# for val in reviews: \n",
    "\n",
    "#     val = str(val) \n",
    "\n",
    "#     tokens = val.split() \n",
    "      \n",
    "#     for i in range(len(tokens)): \n",
    "#         tokens[i] = tokens[i].lower() \n",
    "          \n",
    "#     for words in tokens: \n",
    "#         comment_words = comment_words + words + ' '\n",
    "  \n",
    "  \n",
    "# wordcloud = WordCloud(width = 800, height = 800, \n",
    "#                 background_color ='white', \n",
    "#                 stopwords = stopwords, \n",
    "#                 min_font_size = 10).generate(comment_words) \n",
    "  \n",
    "# plot                       \n",
    "# plt.figure(figsize = (6, 6), facecolor = None) \n",
    "# plt.imshow(wordcloud) \n",
    "# plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing (lowercase + remove punctuation)\n",
    "# reviews = [i.lower() for i in reviews]\n",
    "# reviews = [re.sub(\"[^\\w\\s]\", '', i) for i in reviews]\n",
    "# reviews = [i.replace(\"\\xa0\", '') for i in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize reviews\n",
    "reviews_tk = [nltk.tokenize.SpaceTokenizer().tokenize(i) for i in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize with different n-grams\n",
    "# def ngrams(input, n):\n",
    "#     input = input.split(' ')\n",
    "#     output = []\n",
    "#     for i in range(len(input)-n+1):\n",
    "#         output.append(input[i:i+n])\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(s, n):\n",
    "    # Pre-process\n",
    "    s = [i.lower() for i in s]\n",
    "    s = [re.sub(\"[^\\w\\s]\", '', i) for i in s]\n",
    "    s = [i.replace(\"\\xa0\", '') for i in s]\n",
    "\n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [nltk.tokenize.SpaceTokenizer().tokenize(i) for i in s][0]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "\n",
    "#[\" \".join(ngram) for ngram in ngrams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ngrams(reviews, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find word frequencies and store in dictionary\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "word_frequencies = {}\n",
    "\n",
    "for review in reviews_tk:  \n",
    "    for word in review:\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide word frequencies by max frequency\n",
    "max_freq = max(word_frequencies.values())\n",
    "\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = word_frequencies[word] / max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find review scores of reviews less than 50 words\n",
    "review_scores = {}  \n",
    "for review in reviews:  \n",
    "    for word in nltk.word_tokenize(review.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(review.split(' ')) < 60:\n",
    "                if review not in review_scores.keys():\n",
    "                    review_scores[review] = word_frequencies[word]\n",
    "                else:\n",
    "                    review_scores[review] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_reviews = heapq.nlargest(5, review_scores, key=review_scores.get)\n",
    "\n",
    "summary = '| '.join(summary_reviews)  \n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
